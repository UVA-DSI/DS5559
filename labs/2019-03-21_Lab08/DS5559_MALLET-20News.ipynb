{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "Create an LDA of 20news corpus using MALLET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = '20news_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/usr/local/Cellar/mallet/2.0.8/bin/mallet'\n",
    "num_topics = 15\n",
    "num_iters = 1000\n",
    "show_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import random\n",
    "import textman as tx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pragmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv(src_file, sep='\\t')\n",
    "docs = docs.set_index('doc_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Convert corpus to tokens and vocab\n",
    "\n",
    "We use a function from TextMan, a bespoke library that incorporates the text processing routines used in earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab = tx.create_tokens_and_vocab(docs, src_col='doc_content')\n",
    "tokens['token_num'] = tokens.groupby(['doc_id']).cumcount()\n",
    "tokens = tokens.reset_index()[['doc_id','token_num','term_id']]\n",
    "tokens = tokens[tokens.term_id.isin(vocab[vocab.go].index)]\n",
    "tokens = tokens.set_index(['doc_id','token_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add term strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_str'] = tokens.term_id.map(vocab.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term_id</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">76209</th>\n",
       "      <th>0</th>\n",
       "      <td>4557</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5848</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4713</td>\n",
       "      <td>posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2671</td>\n",
       "      <td>forwarded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5882</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  term_id   term_str\n",
       "doc_id token_num                    \n",
       "76209  0             4557     people\n",
       "       1             5848       sure\n",
       "       2             4713      posts\n",
       "       3             2671  forwarded\n",
       "       4             5882     system"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Remove insignificant words\n",
    "\n",
    "We use SKlearn's TFIDF vectorizor to quicky get a TFIDF vector space, which we use only to filter the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=1, stop_words='english', token_pattern=r'[A-Za-z][A-Za-z][A-Za-z]+')\n",
    "X = vectorizer.fit_transform(docs.doc_content.values.tolist())\n",
    "v = pd.DataFrame(vectorizer.get_feature_names(), columns=['term_str'])\n",
    "v['idf'] = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3491</th>\n",
       "      <td>nicely</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3488</th>\n",
       "      <td>nhlpa</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>nga</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>newswriter</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>newsweek</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>newsgroups</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>newsbytes</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>newly</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_str       idf\n",
       "0            aaa  4.921973\n",
       "3491      nicely  4.921973\n",
       "3488       nhlpa  4.921973\n",
       "3486         nga  4.921973\n",
       "3484  newswriter  4.921973\n",
       "3483    newsweek  4.921973\n",
       "3481   newspaper  4.921973\n",
       "3480  newsgroups  4.921973\n",
       "3478   newsbytes  4.921973\n",
       "3476       newly  4.921973"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.sort_values('idf', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only the most significant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4.5\n",
    "v = v[v.idf > cutoff].sort_values('idf', ascending=False).sample(1000)\n",
    "my_v = v.term_str.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[tokens.term_str.isin(my_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = vocab[vocab.term.isin(my_v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export corpus for MALLET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tx.gather_tokens(tokens, level=0, col='term_str')\\\n",
    "    .reset_index().rename(columns={'term_str':'doc_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20567</td>\n",
       "      <td>atterlep hmmm atheists atheists hmmm atheists ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20607</td>\n",
       "      <td>english easter easter easter english easter en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20741</td>\n",
       "      <td>deeds deeds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20758</td>\n",
       "      <td>orthodox adherents unity unity eternality eter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20859</td>\n",
       "      <td>female desk desk desk desk piece piece female ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                        doc_content\n",
       "0   20567  atterlep hmmm atheists atheists hmmm atheists ...\n",
       "1   20607  english easter easter easter english easter en...\n",
       "2   20741                                        deeds deeds\n",
       "3   20758  orthodox adherents unity unity eternality eter...\n",
       "4   20859  female desk desk desk desk piece piece female ..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('20news-corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized command: \n",
      "Mallet 2.0 commands: \n",
      "\n",
      "  import-dir         load the contents of a directory into mallet instances (one per file)\n",
      "  import-file        load a single file into mallet instances (one per line)\n",
      "  import-svmlight    load SVMLight format data files into Mallet instances\n",
      "  info               get information about Mallet instances\n",
      "  train-classifier   train a classifier from Mallet data files\n",
      "  classify-dir       classify data from a single file with a saved classifier\n",
      "  classify-file      classify the contents of a directory with a saved classifier\n",
      "  classify-svmlight  classify data from a single file in SVMLight format\n",
      "  train-topics       train a topic model from Mallet data files\n",
      "  infer-topics       use a trained topic model to infer topics for new documents\n",
      "  evaluate-topics    estimate the probability of new documents under a trained model\n",
      "  prune              remove features based on frequency or information gain\n",
      "  split              divide data into testing, training, and validation portions\n",
      "  bulk-load          for big input files, efficiently prune vocabulary and import docs\n",
      "\n",
      "Include --help with any option for more information\n"
     ]
    }
   ],
   "source": [
    "!{mallet_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{mallet_path} import-file --input 20news-corpus.csv --output 20news-corpus.mallet --keep-sequence TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mallet LDA: 15 topics, 4 topic bits, 1111 topic mask\n",
      "Data loaded.\n",
      "max tokens: 50\n",
      "total tokens: 471\n",
      "<10> LL/token: -5.27234\n",
      "<20> LL/token: -5.21331\n",
      "<30> LL/token: -5.07567\n",
      "<40> LL/token: -5.04412\n",
      "<50> LL/token: -4.94267\n",
      "<60> LL/token: -4.90945\n",
      "<70> LL/token: -4.87448\n",
      "<80> LL/token: -4.83011\n",
      "<90> LL/token: -4.83432\n",
      "\n",
      "0\t0.33333\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful atheists handle representatives errey \n",
      "1\t0.33333\tjuris date iivx documents caught hmmm wish misc usc gsfc \n",
      "2\t0.33333\tvay mutlu org loving columbia dbd crap success turk biggest criminals atterlep \n",
      "3\t0.33333\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan drivers \n",
      "4\t0.33333\teternality persons concept criticism unity drugs orthodox adherents regard equal biblical stereo prior farzin deeds \n",
      "5\t0.33333\tpitching leadoff straight chimes russotto utkvx boxscores pouring min \n",
      "6\t0.33333\tdet edm que min centris \n",
      "7\t0.33333\tbryn walsh aol hade essay quality asks goer parents napoleon \n",
      "8\t0.33333\tbreton moncton halifax champs salaries cup \n",
      "9\t0.33333\thulman occupied bernadotte prez abc \n",
      "10\t0.33333\tfielding traynor level parents ncsu \n",
      "11\t0.33333\tmemory hst processor gain mss sportster \n",
      "12\t0.33333\tagencies child motss definitions reno raw demanded huji millions agents \n",
      "13\t0.33333\tgirls plenty west piece desk troops army chancellor sites demanded female america orthodox covington \n",
      "14\t0.33333\teaster mass abu english voted den covington org fonts test \n",
      "\n",
      "<100> LL/token: -4.78661\n",
      "<110> LL/token: -4.78285\n",
      "<120> LL/token: -4.7854\n",
      "<130> LL/token: -4.78673\n",
      "<140> LL/token: -4.71261\n",
      "<150> LL/token: -4.69256\n",
      "<160> LL/token: -4.66893\n",
      "<170> LL/token: -4.66861\n",
      "<180> LL/token: -4.71668\n",
      "<190> LL/token: -4.696\n",
      "\n",
      "0\t0.33333\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful representatives handle plenty demanded huji \n",
      "1\t0.33333\tjuris date documents hulman wish hst stereo hmmm misc usc biggest sportster \n",
      "2\t0.33333\tvay mutlu org loving columbia dbd crap success turk biggest criminals \n",
      "3\t0.33333\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan salaries \n",
      "4\t0.33333\teternality persons orthodox concept criticism unity adherents regard equal biblical handle agents abc deeds \n",
      "5\t0.33333\tpitching piece desk leadoff straight russotto utkvx napoleon pouring \n",
      "6\t0.33333\tdet edm que min \n",
      "7\t0.33333\tbryn hade den asks goer gain \n",
      "8\t0.33333\tbreton moncton halifax champs errey ncsu cup centris gsfc atterlep \n",
      "9\t0.33333\tbernadotte reno essay boxscores farzin drivers caught covington \n",
      "10\t0.33333\tfielding iivx level traynor raw female \n",
      "11\t0.33333\tmemory plenty chimes processor mss prior fonts \n",
      "12\t0.33333\tagencies drugs aol prez definitions caught quality millions test \n",
      "13\t0.33333\tgirls west occupied mass troops abu parents army voted sites demanded america \n",
      "14\t0.33333\teaster walsh child english atheists motss chancellor covington hmmm \n",
      "\n",
      "<200> LL/token: -4.74599\n",
      "<210> LL/token: -4.65035\n",
      "<220> LL/token: -4.70114\n",
      "<230> LL/token: -4.70276\n",
      "<240> LL/token: -4.68745\n",
      "<250> LL/token: -4.69937\n",
      "<260> LL/token: -4.74395\n",
      "<270> LL/token: -4.70632\n",
      "<280> LL/token: -4.66421\n",
      "<290> LL/token: -4.70133\n",
      "\n",
      "0\t0.33333\tlevel decisions reasoning concentrate arguing vetos senators papers authority capable powerful handle representatives boxscores \n",
      "1\t0.33333\tjuris date documents hmmm wish misc usc agents abc \n",
      "2\t0.33333\tvay mutlu org loving columbia dbd crap biggest success goer \n",
      "3\t0.33333\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan \n",
      "4\t0.33333\teternality persons orthodox concept criticism unity adherents regard equal biblical deeds \n",
      "5\t0.33333\tpitching plenty piece desk leadoff straight turk utkvx napoleon huji fonts \n",
      "6\t0.33333\tdet edm que min russotto \n",
      "7\t0.33333\tbryn hade den errey ncsu memory gsfc \n",
      "8\t0.33333\tbreton moncton halifax champs cup \n",
      "9\t0.33333\thulman bernadotte chimes aol hst asks prior success sportster \n",
      "10\t0.33333\tfielding iivx drugs caught traynor raw farzin \n",
      "11\t0.33333\tmemory processor essay salaries criminals female english atterlep \n",
      "12\t0.33333\tagencies walsh prez voted definitions reno stereo quality mss millions test \n",
      "13\t0.33333\tgirls west occupied mass demanded troops abu parents army gain america aol drivers centris female \n",
      "14\t0.33333\teaster child covington atheists motss chancellor sites english pouring \n",
      "\n",
      "[beta: 0.02815] \n",
      "<300> LL/token: -4.52652\n",
      "<310> LL/token: -4.43513\n",
      "<320> LL/token: -4.49592\n",
      "<330> LL/token: -4.47057\n",
      "<340> LL/token: -4.46676\n",
      "<350> LL/token: -4.477\n",
      "<360> LL/token: -4.45587\n",
      "<370> LL/token: -4.4436\n",
      "<380> LL/token: -4.47727\n",
      "<390> LL/token: -4.47677\n",
      "\n",
      "0\t0.14995\tdecisions level reasoning arguing vetos senators papers authority capable powerful concentrate handle representatives salaries piece \n",
      "1\t0.1073\tjuris date documents wish voted hmmm usc occupied biggest \n",
      "2\t0.14103\tvay mutlu org loving columbia dbd crap success turk desk biggest female criminals \n",
      "3\t0.12116\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan napoleon \n",
      "4\t0.10048\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.23959\tpitching leadoff straight child piece stereo desk atterlep hmmm \n",
      "6\t0.16496\tedm que det min \n",
      "7\t0.17556\tbryn iivx hade atheists den det hulman huji ncsu centris \n",
      "8\t0.11553\tbreton moncton halifax champs motss goer cup \n",
      "9\t0.19744\tmemory plenty chimes processor misc mss \n",
      "10\t0.21711\tfielding traynor level hst gain boxscores sportster army drivers abc min asks \n",
      "11\t0.19246\tdrugs walsh aol caught russotto raw utkvx quality pouring fonts gsfc test orthodox \n",
      "12\t0.15114\tagencies bernadotte prez definitions reno farzin millions \n",
      "13\t0.21116\tgirls west mass demanded troops abu parents hulman sites occupied army errey america org asks \n",
      "14\t0.21096\teaster covington english chancellor essay prior concentrate agents \n",
      "\n",
      "[beta: 0.0277] \n",
      "<400> LL/token: -4.33888\n",
      "<410> LL/token: -4.33625\n",
      "<420> LL/token: -4.32848\n",
      "<430> LL/token: -4.31861\n",
      "<440> LL/token: -4.31519\n",
      "<450> LL/token: -4.3332\n",
      "<460> LL/token: -4.35226\n",
      "<470> LL/token: -4.30978\n",
      "<480> LL/token: -4.31874\n",
      "<490> LL/token: -4.31349\n",
      "\n",
      "0\t0.08432\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful handle representatives asks plenty demanded piece \n",
      "1\t0.04689\tjuris date iivx documents wish hmmm misc usc biggest centris \n",
      "2\t0.06423\tvay mutlu org loving columbia dbd desk crap turk piece success biggest criminals \n",
      "3\t0.06316\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan atterlep \n",
      "4\t0.03103\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.14604\tpitching leadoff straight voted chancellor hst farzin caught fonts hmmm \n",
      "6\t0.08088\tdet edm que min abc test \n",
      "7\t0.07112\tbryn hade atheists den ncsu drivers gsfc \n",
      "8\t0.06554\tbreton moncton halifax champs goer cup \n",
      "9\t0.10792\tmemory plenty chimes russotto processor mss prior success \n",
      "10\t0.11164\tfielding child motss traynor level sportster agents \n",
      "11\t0.12559\tdrugs walsh covington raw caught essay aol \n",
      "12\t0.09049\tagencies bernadotte prez english aol definitions reno gain salaries huji millions \n",
      "13\t0.12685\tgirls west occupied mass troops abu parents army sites demanded quality female america org pouring orthodox \n",
      "14\t0.12244\teaster hulman utkvx stereo errey boxscores napoleon \n",
      "\n",
      "[beta: 0.02899] \n",
      "<500> LL/token: -4.26584\n",
      "<510> LL/token: -4.21967\n",
      "<520> LL/token: -4.22227\n",
      "<530> LL/token: -4.33761\n",
      "<540> LL/token: -4.23957\n",
      "<550> LL/token: -4.20899\n",
      "<560> LL/token: -4.27641\n",
      "<570> LL/token: -4.22559\n",
      "<580> LL/token: -4.22713\n",
      "<590> LL/token: -4.23965\n",
      "\n",
      "0\t0.04116\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful handle representatives asks plenty demanded agents piece \n",
      "1\t0.03449\tjuris date iivx documents wish hmmm misc usc biggest \n",
      "2\t0.03374\tvay mutlu org loving columbia dbd desk crap success turk piece biggest criminals female \n",
      "3\t0.02782\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan \n",
      "4\t0.01722\teternality persons concept criticism unity orthodox adherents regard equal biblical occupied deeds \n",
      "5\t0.07807\tpitching leadoff straight atheists voted chancellor huji ncsu fonts gsfc atterlep hmmm \n",
      "6\t0.04079\tedm que min det gain drivers \n",
      "7\t0.04993\tbryn hade hst den det sportster \n",
      "8\t0.04165\tbreton moncton halifax champs goer salaries farzin cup \n",
      "9\t0.0745\tmemory chimes russotto utkvx processor errey \n",
      "10\t0.04945\tfielding hulman traynor level boxscores napoleon centris \n",
      "11\t0.08161\tdrugs walsh plenty caught raw essay quality mss prior test \n",
      "12\t0.04127\tagencies child prez motss definitions reno millions \n",
      "13\t0.07688\tgirls west aol mass troops abu parents army sites occupied demanded america org female orthodox \n",
      "14\t0.07702\teaster bernadotte covington english stereo pouring abc \n",
      "\n",
      "[beta: 0.02176] \n",
      "<600> LL/token: -4.22102\n",
      "<610> LL/token: -4.20313\n",
      "<620> LL/token: -4.20188\n",
      "<630> LL/token: -4.24863\n",
      "<640> LL/token: -4.19693\n",
      "<650> LL/token: -4.23404\n",
      "<660> LL/token: -4.19781\n",
      "<670> LL/token: -4.22521\n",
      "<680> LL/token: -4.22433\n",
      "<690> LL/token: -4.24664\n",
      "\n",
      "0\t0.02756\tdecisions level reasoning plenty concentrate arguing vetos senators papers authority capable powerful handle representatives asks demanded success piece \n",
      "1\t0.02402\tjuris date iivx documents hmmm wish misc usc biggest agents centris \n",
      "2\t0.02354\tvay mutlu org loving columbia dbd desk crap turk piece success biggest criminals female \n",
      "3\t0.02096\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan gain \n",
      "4\t0.00881\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.05343\tpitching leadoff straight voted chancellor hulman boxscores caught gsfc \n",
      "6\t0.02955\tedm que min det salaries abc \n",
      "7\t0.0226\tbryn hade den det \n",
      "8\t0.02179\tbreton moncton champs halifax cup \n",
      "9\t0.0431\tmemory chimes russotto atheists hst utkvx processor goer mss prior huji sportster halifax atterlep \n",
      "10\t0.02996\tfielding traynor level stereo errey farzin napoleon \n",
      "11\t0.05705\tdrugs walsh child motss aol raw caught quality test \n",
      "12\t0.02552\tagencies prez definitions reno ncsu millions pouring \n",
      "13\t0.04839\tgirls west occupied mass troops abu parents army sites demanded hulman america aol org drivers fonts female orthodox \n",
      "14\t0.04816\teaster bernadotte covington english essay \n",
      "\n",
      "[beta: 0.02885] \n",
      "<700> LL/token: -4.22714\n",
      "<710> LL/token: -4.17769\n",
      "<720> LL/token: -4.18887\n",
      "<730> LL/token: -4.1643\n",
      "<740> LL/token: -4.22469\n",
      "<750> LL/token: -4.17961\n",
      "<760> LL/token: -4.17317\n",
      "<770> LL/token: -4.17747\n",
      "<780> LL/token: -4.15898\n",
      "<790> LL/token: -4.15323\n",
      "\n",
      "0\t0.02602\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful handle representatives asks gain plenty demanded agents fonts piece \n",
      "1\t0.01538\tjuris date iivx documents wish hmmm usc biggest centris \n",
      "2\t0.01664\tvay mutlu org loving columbia dbd desk crap turk piece success biggest criminals female \n",
      "3\t0.01317\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan \n",
      "4\t0.00918\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.03653\tpitching leadoff straight hulman caught voted chancellor boxscores west pouring drivers \n",
      "6\t0.01947\tedm que det min napoleon \n",
      "7\t0.01723\tbryn hade den det errey farzin \n",
      "8\t0.01451\tbreton moncton halifax champs goer cup \n",
      "9\t0.03158\tmemory chimes atheists hst processor sportster hmmm \n",
      "10\t0.02541\tfielding russotto traynor level utkvx stereo misc min \n",
      "11\t0.04315\tdrugs walsh child motss aol raw quality abc gsfc test \n",
      "12\t0.01171\tagencies prez definitions reno ncsu millions \n",
      "13\t0.03305\tgirls occupied mass troops abu parents west army sites demanded salaries america aol org huji female orthodox \n",
      "14\t0.02957\teaster plenty bernadotte covington english essay mss prior success \n",
      "\n",
      "[beta: 0.02633] \n",
      "<800> LL/token: -4.19103\n",
      "<810> LL/token: -4.16752\n",
      "<820> LL/token: -4.20857\n",
      "<830> LL/token: -4.19725\n",
      "<840> LL/token: -4.17224\n",
      "<850> LL/token: -4.21206\n",
      "<860> LL/token: -4.16519\n",
      "<870> LL/token: -4.20692\n",
      "<880> LL/token: -4.15418\n",
      "<890> LL/token: -4.15533\n",
      "\n",
      "0\t0.01818\tdecisions level reasoning arguing vetos senators papers authority capable powerful concentrate handle representatives plenty demanded success napoleon asks piece \n",
      "1\t0.01104\tjuris date iivx documents hmmm atheists wish gain misc usc biggest farzin centris atterlep \n",
      "2\t0.01204\tvay mutlu org loving columbia dbd desk crap turk piece success biggest criminals female \n",
      "3\t0.00971\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan \n",
      "4\t0.00549\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.03002\tpitching leadoff straight voted chancellor hulman errey boxscores caught \n",
      "6\t0.01157\tedm que det min abc \n",
      "7\t0.01043\tbryn hade hst den det sportster \n",
      "8\t0.01088\tbreton moncton halifax champs essay goer cup covington \n",
      "9\t0.03095\tmemory chimes processor fonts \n",
      "10\t0.02092\tfielding russotto traynor level utkvx concentrate agents min asks \n",
      "11\t0.03021\tdrugs walsh child motss aol raw caught stereo quality salaries huji ncsu \n",
      "12\t0.00637\tagencies prez definitions reno millions test \n",
      "13\t0.02485\tgirls west occupied mass troops abu parents sites demanded army hulman america aol org female orthodox \n",
      "14\t0.0236\teaster plenty bernadotte english covington mss prior pouring army drivers gsfc \n",
      "\n",
      "[beta: 0.01553] \n",
      "<900> LL/token: -4.22859\n",
      "<910> LL/token: -4.2563\n",
      "<920> LL/token: -4.24581\n",
      "<930> LL/token: -4.24257\n",
      "<940> LL/token: -4.22576\n",
      "<950> LL/token: -4.24137\n",
      "<960> LL/token: -4.21932\n",
      "<970> LL/token: -4.24537\n",
      "<980> LL/token: -4.21895\n",
      "<990> LL/token: -4.24377\n",
      "\n",
      "0\t0.01485\tdecisions level reasoning concentrate arguing vetos senators papers authority capable powerful handle representatives asks plenty demanded ncsu agents piece \n",
      "1\t0.01217\tjuris date iivx documents hmmm atheists wish misc usc biggest centris atterlep \n",
      "2\t0.00894\tvay mutlu org loving columbia dbd desk crap turk piece success biggest criminals female \n",
      "3\t0.00961\tkarabag bayonetted huseyin agdam reactions village slaughter cold azerbaijan \n",
      "4\t0.00404\teternality persons concept criticism unity orthodox adherents regard equal biblical deeds \n",
      "5\t0.02316\tpitching leadoff straight caught chancellor farzin fonts \n",
      "6\t0.00814\tedm que det min abc \n",
      "7\t0.00696\tbryn hulman hade hst den det boxscores sportster \n",
      "8\t0.00991\tbreton moncton halifax champs goer cup \n",
      "9\t0.02043\tmemory chimes processor gain errey salaries huji \n",
      "10\t0.01519\tfielding russotto traynor level utkvx pouring army drivers min \n",
      "11\t0.02453\tdrugs walsh child motss voted aol raw stereo quality chancellor napoleon test \n",
      "12\t0.00511\tagencies prez definitions reno millions gsfc \n",
      "13\t0.01743\tgirls west occupied mass troops abu parents sites demanded army america aol org female orthodox \n",
      "14\t0.02006\teaster plenty bernadotte covington english essay mss prior success \n",
      "\n",
      "[beta: 0.01525] \n",
      "<1000> LL/token: -4.23903\n",
      "\n",
      "Total time: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "!{mallet_path} train-topics --input 20news-corpus.mallet --num-topics {num_topics} --num-iterations {num_iters} \\\n",
    "--output-doc-topics 20news-doc-topics.txt \\\n",
    "--output-topic-keys 20news-topic-keys.txt \\\n",
    "--word-topic-counts-file 20news-word-topic-counts-file.txt \\\n",
    "--topic-word-weights-file 20news-topic-word-weights-file.txt \\\n",
    "--xml-topic-report 20news-topic-report.xml \\\n",
    "--xml-topic-phrase-report 20news-topic-phrase-report.xml \\\n",
    "--show-topics-interval {show_interval} \\\n",
    "--use-symmetric-alpha false  \\\n",
    "--optimize-interval 100 \\\n",
    "--diagnostics-file 20news-diagnostics.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "789px",
    "left": "0px",
    "right": "1186px",
    "top": "111px",
    "width": "254px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
