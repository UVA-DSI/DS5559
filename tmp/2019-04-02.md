---
theme : 'white'
customTheme : 'default'
transition: 'slide'
title : 'Word Embedding'
---

# Word Embedding

**Raf Alvarado**  
UVA DS 5559  
02 April 2019

---

## Business

---

## Review

* Topic models useful in studying culture and history
  * Themes in literature, political and cultural change
* **Heteroglossia**, a key concept in socipology of language, indexed by **topic mixtures** ($\theta_{d}$)
  * **KLD between documents** (as topic mixtures) can be used to describe linguistic change
  * **Mutual information between topics** in topic mixtures can be used to surface **cultural networks**
* All methods derive from $\phi$ and $\theta$ tables
  * Produced by a tools like `MALLET`

---

# Word Embedding

---

## What is word embedding?

* A class of unsupervised algorithms for learning the meanings of words
* Based on the vector representations of words in contexts
  * Contexts = 'embeddings'
  * We have already seen this (From Frequency to Meaning)
  * Topic models also provide implicit embeddings -- topics as contexts
* These vectors allow a semantic algebra
  * $vec(king) + vec(male) - vec(queen) ~= vec(female)$


---

## The Distributional Hypothesis

It is based on the distributional hypothesis

Recall the problem of meaning ... Shannon, from Frequency to Meaning

Words that occur in the same contexts tend to have similar meanings (Harris 1954)

> "a word is characterized by the company it keeps" (Firth 1957)

Also posited by Wittgenstein, Weaver, others.

Basis for Statistical Semantics

---

## Mayan Decipherment

![](images/ahaw.png)

These signs all mean the same  thing

Their similarity was discovered through their common subsitution patterns

---

## Ontology of Text Analytics

* Words
* Topics
* Documents

---

## Word Vectors and Features

* Already seen in Doc-Term and Topic-Term matrices
* Word - Word matrix
* Components

---

## Methods

* PPMI
* SVD
* SGNS -- word2vec
* GloVe
---

## Uses for Word Embedding

* More features for training data
* Exploration of semantic networks
* Study of analogy -- culture built on systems of analogical classification
* Semantic change (historical linguistics)
---

## Semantic Shifts

![](images/word-vectors-over-time.png)

Hamilton, Leskovec, and Jurafsky

Law of conformity

Law of innovation

---

## Represenation

* CBOW and Skipgrams

---

## Data Process

* Transforms from tokens table

--

```sql
WITH mytoken(author, book, chapter, para_num, sent_num,token_num,term_str,term_id) 
    AS (
        SELECT author, book, chapter, para_num, sent_num,token_num,term_str,term_id
        FROM token 
        WHERE term_id IN (SELECT term_id FROM vocab WHERE stop = 0) 
            AND term_str is not NULL
            AND pos NOT LIKE 'NNP_'
    )
SELECT x.term_str as core, y.term_str as partner, (y.token_num - x.token_num) AS dist
FROM mytoken x JOIN mytoken y USING(author, book, chapter, para_num, sent_num)
WHERE y.token_num IN (x.token_num - 2, x.token_num - 1, x.token_num + 1, x.token_num + 2)
ORDER BY core, dist, partner
```

SQL to covert tokens table into CBOW

---

## Use of Neural Networks

---

## Similarity

cosine

---

## Additive Compositionality

Vector Semantics

A + B - D = C

---

## Analogical Classification

```text
                             +
                        A ------- B
                             |
                             |       -
                             |
                        C ------- D
```

```text
A, C: Subjects   --> S1, S2
B, D: Predicates --> P1, P2

S1 - P1 + P2 = S2
```

--

`A : B -> S V O`

`A : B :: C : D -> S1 + V1 + O1 = S2 + V2 + O2`

 `V1 = V2`

`S1 + O1 = S2 + O2`

`S1 + O1 - O2 = S2`

`A  + B  - D  = C`

---

## Implementations

* word2vec
* gloVe

---

## word2vec

Developed by Tomáš Mikolov with colleagues at Google in 2013

> ... an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words. These representations can be subsequently used in many natural language processing applications and for further research.

See https://code.google.com/archive/p/word2vec/

---

## Visualization

---

## Resources

* [Jupyter Notebook implementation](https://github.com/ujhuyz0110/wrd_emb/blob/master/word2vec_skipgram_medium_v1.ipynb)
* [Implement Your Own Word2Vec Skipgram Model](https://www.geeksforgeeks.org/implement-your-own-word2vecskip-gram-model-in-python/)
* [Making Sense of  Word2Vec](https://rare-technologies.com/making-sense-of-word2vec/)
* [Word2Vec from Scratch with Numpy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)